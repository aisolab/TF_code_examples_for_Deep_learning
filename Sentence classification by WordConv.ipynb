{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence classification by WordConv\n",
    "Implementation of [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) to classify sentiment of movie review\n",
    "\n",
    "### Explanation of this notebook\n",
    "* Dataset : [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc)\n",
    "* Preprocessing\n",
    "    + Morphological analysis by Mecab wrapped by [konlpy](http://konlpy.org/en/latest/)\n",
    "    + Using [FastText](https://arxiv.org/abs/1607.04606) embedding by [gluonnlp package](https://gluon-nlp.mxnet.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import konlpy\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from tensorflow import keras\n",
    "from pprint import pprint\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199992, 2)\n"
     ]
    }
   ],
   "source": [
    "rating = pd.read_csv('./ratings.txt', sep = '\\t')[['document', 'label']]\n",
    "\n",
    "# document 내용이 없는 문서는 제거\n",
    "rating = rating.loc[map(lambda elm : not elm, rating.document.isna()),:]\n",
    "print(rating.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting train, validation, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = konlpy.tag.Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '1'],\n",
      "       ['디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.',\n",
      "        '1'],\n",
      "       ['폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.', '1'],\n",
      "       ['와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지', '1'],\n",
      "       ['안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.', '1']], dtype='<U142')\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([[doc, label] for doc, label in zip(rating.document, rating.label)])\n",
    "pprint(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143995, 2) (35998, 2) (19999, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting train, validation, test\n",
    "tst_indices = np.random.choice(range(len(dataset)),\n",
    "                               size = int(len(dataset) * .1),\n",
    "                               replace = False)\n",
    "tst_dataset = dataset[tst_indices]\n",
    "training_dataset = np.delete(dataset, tst_indices, axis = 0)    \n",
    "\n",
    "val_indices = np.random.choice(range(len(training_dataset)),\n",
    "                               size = int(len(training_dataset) * .2),\n",
    "                               replace = False)\n",
    "val_dataset = training_dataset[val_indices]\n",
    "tr_dataset = np.delete(training_dataset, val_indices, axis = 0)\n",
    "\n",
    "tr_dataset = tr_dataset.tolist()\n",
    "val_dataset = val_dataset.tolist()\n",
    "tst_dataset = tst_dataset.tolist()\n",
    "print(np.shape(tr_dataset), np.shape(val_dataset), np.shape(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_tr = list(map(lambda sen : mecab.morphs(sen[0]), tr_dataset))\n",
    "y_tr = list(map(lambda sen : int(sen[1]), tr_dataset))\n",
    "\n",
    "# validation\n",
    "X_val = list(map(lambda sen : mecab.morphs(sen[0]), val_dataset))\n",
    "y_val = list(map(lambda sen : int(sen[1]), val_dataset))\n",
    "\n",
    "# test\n",
    "X_tst = list(map(lambda sen : mecab.morphs(sen[0]), tst_dataset))\n",
    "y_tst = list(map(lambda sen : int(sen[1]), tst_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building vocabulary and connecting vocab with fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr에 대한 vocab 생성\n",
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c in X_tr]))\n",
    "vocab = nlp.Vocab(counter,bos_token=None, eos_token=None, min_freq=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fasttext embedding \n",
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko')\n",
    "\n",
    "# vocab에 embedding 연결\n",
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final preprocessing\n",
    "X_tr = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], X_tr))\n",
    "X_tr = keras.preprocessing.sequence.pad_sequences(sequences = X_tr, maxlen = 30, padding = 'post',\n",
    "                                                  value = 1.)\n",
    "\n",
    "X_val = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], X_val))\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(sequences = X_val, maxlen = 30, padding = 'post',\n",
    "                                                   value = 1.)\n",
    "\n",
    "X_tst = list(map(lambda sen : [vocab.token_to_idx[token] for token in sen], X_tst))\n",
    "X_tst = keras.preprocessing.sequence.pad_sequences(sequences = X_tst, maxlen = 30, padding = 'post',\n",
    "                                                   value = 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define WordConv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordConv:\n",
    "    def __init__(self, X, y, n_of_classes, embedding):\n",
    "        \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            self._X = X\n",
    "            self._y = y\n",
    "            self._is_training = tf.placeholder(dtype = tf.bool)\n",
    "        \n",
    "        with tf.variable_scope('embedding_layer'):\n",
    "            static_embed = tf.get_variable(name = 'static', initializer = embedding,\n",
    "                                     trainable = True)\n",
    "            non_static_embed = tf.get_variable(name = 'non_static', initializer = embedding,\n",
    "                                        trainable = False)\n",
    "            static_batch = tf.nn.embedding_lookup(params = static_embed, ids = self._X)\n",
    "            non_static_batch = tf.nn.embedding_lookup(params = non_static_embed, ids = self._X)\n",
    "            \n",
    "        with tf.variable_scope('convoluion_layer', reuse = tf.AUTO_REUSE):\n",
    "            with slim.arg_scope([slim.conv1d], num_outputs = 100, padding = 'VALID'):\n",
    "                \n",
    "                static_3 = slim.conv1d(inputs = static_batch, kernel_size = 3, scope = '3_gram')\n",
    "                non_static_3 = slim.conv1d(inputs = non_static_batch, kernel_size = 3, scope = '3_gram')\n",
    "\n",
    "                static_4 = slim.conv1d(inputs = static_batch, kernel_size = 4, scope = '4_gram')\n",
    "                non_static_4 = slim.conv1d(inputs = non_static_batch, kernel_size = 4, scope = '4_gram')\n",
    "\n",
    "                static_5 = slim.conv1d(inputs = static_batch, kernel_size = 5, scope = '5_gram')\n",
    "                non_static_5 = slim.conv1d(inputs = non_static_batch, kernel_size = 5, scope = '5_gram')\n",
    "\n",
    "            fmap_3 = tf.reduce_max(static_3 + non_static_3, axis = 1)\n",
    "            fmap_4 = tf.reduce_max(static_4 + non_static_4, axis = 1)\n",
    "            fmap_5 = tf.reduce_max(static_5 + non_static_5, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('output_layer'):\n",
    "            flattened = tf.concat([fmap_3, fmap_4, fmap_5], axis = -1)\n",
    "            score = slim.fully_connected(inputs = flattened, num_outputs = n_of_classes,\n",
    "                                         weights_regularizer = slim.l2_regularizer(.05),\n",
    "                                         activation_fn = None)\n",
    "            self._score = slim.dropout(inputs = score, is_training = self._is_training)\n",
    "            \n",
    "        with tf.variable_scope('loss'):\n",
    "            ce_loss = tf.losses.sparse_softmax_cross_entropy(labels = self._y, logits = self._score)\n",
    "            reg_term = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            self.total_loss = ce_loss + reg_term\n",
    "        \n",
    "        with tf.variable_scope('prediction'):\n",
    "            self._prediction = tf.argmax(self._score, axis = -1)\n",
    "        \n",
    "        def predict(self, sess, x_data, is_training = False):\n",
    "            feed_prediction = {self._X : x_data, self._is_training : is_training}\n",
    "            return sess.run(self._prediction, feed_dict = feed_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of WordConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "lr = .003\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "total_step = int(X_tr.shape[0] / batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 1000000)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "tr_iterator = tr_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size = batch_size)\n",
    "val_iterator = val_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymous iterator\n",
    "handle = tf.placeholder(dtype = tf.string)\n",
    "iterator = tf.data.Iterator.from_string_handle(string_handle = handle,\n",
    "                                               output_types = tr_iterator.output_types,\n",
    "                                               output_shapes = tr_iterator.output_shapes)\n",
    "x_data, y_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_conv = WordConv(X = x_data,\n",
    "                     y = y_data,\n",
    "                     n_of_classes=2,\n",
    "                     embedding = vocab.embedding.idx_to_vec.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training op\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = word_conv.total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config = sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tr_handle, val_handle = sess.run(fetches = [tr_iterator.string_handle(), val_iterator.string_handle()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, tr_loss : 0.635, val_loss : 0.396\n",
      "epoch :   2, tr_loss : 0.455, val_loss : 0.385\n",
      "epoch :   3, tr_loss : 0.406, val_loss : 0.329\n",
      "epoch :   4, tr_loss : 0.398, val_loss : 0.330\n",
      "epoch :   5, tr_loss : 0.372, val_loss : 0.353\n",
      "CPU times: user 1h 20min 11s, sys: 5min 2s, total: 1h 25min 13s\n",
      "Wall time: 11min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tr_loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    avg_tr_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    tr_step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    # for mini-batch training\n",
    "    sess.run(tr_iterator.initializer)    \n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "            _, tr_loss = sess.run(fetches = [training_op, word_conv.total_loss],\n",
    "                                             feed_dict = {handle : tr_handle, word_conv._is_training : True})\n",
    "            avg_tr_loss += tr_loss\n",
    "            tr_step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    # for validation\n",
    "    sess.run(val_iterator.initializer)\n",
    "    try:\n",
    "        while True:\n",
    "            val_loss = sess.run(fetches = word_conv.total_loss,\n",
    "                                feed_dict = {handle : val_handle, word_conv._is_training : False})\n",
    "            avg_val_loss += val_loss\n",
    "            val_step += 1\n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    avg_tr_loss /= tr_step\n",
    "    avg_val_loss /= val_step\n",
    "    tr_loss_hist.append(avg_tr_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    \n",
    "    print('epoch : {:3}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch + 1, avg_tr_loss, avg_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = tf.data.Dataset.from_tensor_slices((X_tst, y_tst))\n",
    "tst_dataset = tst_dataset.batch(batch_size = batch_size)\n",
    "tst_iterator = tst_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_handle = sess.run(tst_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tst_hat = np.array([])\n",
    "\n",
    "sess.run(tst_iterator.initializer)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        y_tst_tmp = sess.run(word_conv._prediction,\n",
    "                            feed_dict = {handle : tst_handle,\n",
    "                                         word_conv._is_training : False})\n",
    "        y_tst_hat= np.append(y_tst_hat,y_tst_tmp)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc : 84.87%\n"
     ]
    }
   ],
   "source": [
    "print('test acc : {:.2%}'.format(np.mean(y_tst_hat == np.array(y_tst))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
