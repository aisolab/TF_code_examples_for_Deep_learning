{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of implementing Sequence classification with RNN series\n",
    "Sequence를 modeling 할 수 있는 RNN series에는 기본적인 Recurrent Neural Network, Long Short-Term Memory Units (LSTM), Gated Recurrent Unit (GRU) 등 여러 Cell을 활용할 수 있지만, ***이 Tutorial에서는 many to one의  예제로 영어단어의 알파벳을 하나씩보고 (영어단어의 길이는 모두 다르다.), 단어의 긍/부정을 예측하는 Character level RNN을 학습한다. cell은 GRU을 활용한다.*** 특히 알파벳을 벡터로 표현하는 방법은 아래의 논문에서 소개된 ***Character quantization*** 방법을 참고하였다.   \n",
    "\n",
    "* Paper : https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf \n",
    "* Reference : https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/02%20-%20Autocomplete.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charater quantization\n",
    "Paper에서는 영어로 쓰인 어떤 글이라 One hot vector의 모음으로 표현할 수 있도록, 아래와 같은 문자열들을 모두 one hot encoding에 포함시키지만 ***이 Tutorial에서는 영어 알파벳만 활용한다.***\n",
    "\n",
    "Paper : \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = \"abcdefghijklmnopqrstuvwxyz \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dic = {char : idx for idx, char in enumerate(char_arr)}\n",
    "dic_len = len(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 26,\n",
       " 'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char_dic을 이용해서 영어알파벳을 27차원의 one hot vector로 만들 수 있다.\n",
    "char_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' '])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define user function\n",
    "***GRU의 input을 생성하는 make_batch function***을 작성한다. max_len 이라는 argument가 존재하는 이유는 RNN은 실제로 variable length sequence를 처리할 수 있지만, Tensorflow에서는 RNN의 input의 sequence는 고정된 길이로 주어야하기 때문에, 일단 max_len의 길이만큼 padding을 한다. 코드로 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, max_len):\n",
    "    seq_len = []\n",
    "    seq_batch = []\n",
    "    for seq in seq_data:\n",
    "        seq_len.append(len(seq))\n",
    "        seq_idx = [char_dic.get(char) for char in seq]\n",
    "        seq_idx += (max_len - len(seq_idx)) * [0]\n",
    "        seq_matrix = np.eye(dic_len)[seq_idx].tolist()\n",
    "        seq_batch.append(seq_matrix)        \n",
    "    return seq_len, seq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : [4], \n",
      " batch : [[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  1.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# padding을 안했을 시\n",
    "test_len, test_batch =  make_batch(['test'], len('test'))\n",
    "print('length : {}, \\n batch : {}'.format(test_len, np.array(test_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최대 길이를 10으로하면 아래와 같이 길이가 4인 \"test\"라는 단어에 대해서 우리가 생각한 것과 batch가 다르게 생성되지만, ***해당 문자열의 실제 길이를 저장한 후, tf.placehoder를 이용해서 tf.nn.dynamic_rnn에 sequence_length argument에 값을 전달하여, variable sequence length를 처리할 수 있다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : [4], \n",
      " batch : [[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# max_len (최대길이)를 10으로 padding을 했을 시 \n",
    "test_len, test_batch =  make_batch(['test'], 10)\n",
    "print('length : {}, \\n batch : {}'.format(test_len, np.array(test_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CharRNN class\n",
    "가변길이의 영어단어가 긍정 단어인지 부정단어인지 예측하는 CharRNN 모형을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN():\n",
    "    def __init__(self, n_label, max_len, input_dim, hidden_dim):\n",
    "        with tf.variable_scope('input_layer'):\n",
    "            self._x_len = tf.placeholder(dtype = tf.int32)\n",
    "            self._x_batch = tf.placeholder(dtype = tf.float32, shape = [None, max_len, input_dim])\n",
    "            self._y = tf.placeholder(dtype = tf.float32, shape = [None, n_label])\n",
    "        \n",
    "        with tf.variable_scope('gru_cell'):\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units = hidden_dim)\n",
    "            _, self._hidden = tf.nn.dynamic_rnn(cell = cell, inputs = self._x_batch,\n",
    "                                                sequence_length = self._x_len, dtype = tf.float32)\n",
    "\n",
    "        with tf.variable_scope('output_layer'):\n",
    "            self._score = tf.layers.dense(self._hidden, units = n_label)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            self._ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self._score, labels = self._y))\n",
    "        \n",
    "        with tf.variable_scope('predict'):\n",
    "            self._prediction = tf.argmax(input = self._score, axis = 1)\n",
    "        \n",
    "    def predict(self, sess, seq_len, seq_batch):\n",
    "        feed_predict = {self._x_len : seq_len, self._x_batch : seq_batch}\n",
    "        return sess.run(fetches = self._prediction, feed_dict = feed_predict)\n",
    "    \n",
    "    def encode(self, sess, seq_len, seq_batch):\n",
    "        feed_encode = {self._x_len : seq_len, self._x_batch : seq_batch}\n",
    "        return sess.run(fetches = self._hidden, feed_dict = feed_encode)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Solver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, model, optimizer = tf.train.AdamOptimizer, var_list = None):\n",
    "        self._model = model\n",
    "        self._lr = tf.placeholder(dtype = tf.float32)\n",
    "        self._optimizer = optimizer(learning_rate = self._lr)\n",
    "        self._training_op = self._optimizer.minimize(loss = self._model._ce_loss, var_list = var_list)\n",
    "    \n",
    "    def train(self, sess, seq_len, seq_batch, y_data, lr):\n",
    "        feed_train = {self._model._x_len : seq_len, self._model._x_batch : seq_batch,\n",
    "                      self._model._y : y_data, self._lr : lr}\n",
    "        return sess.run(fetches = [self._training_op, self._model._ce_loss], feed_dict = feed_train)\n",
    "            \n",
    "    def evaluate(self, sess, seq_len, seq_batch, y_data):\n",
    "        feed_loss = {self._model._x_len : seq_len, self._model._x_batch : seq_batch, self._model._y : y_data}\n",
    "        return sess.run(fetches = self._model._ce_loss, feed_dict = feed_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : word sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = [[1.,0.], [0.,1.], [1.,0.], [1., 0.],[0.,1.]]\n",
    "words = ['good', 'bad', 'amazing', 'so good', 'bull shit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words를 CharRNN이 받을 수 있는 데이터로 변환\n",
    "words_len, word_batch = make_batch(words, max_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 7, 7, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "char_gru = CharRNN(n_label = 2, max_len = 10, input_dim = len(char_dic), hidden_dim = 20)\n",
    "adam_solver = Solver(model = char_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "n_epochs = 20\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs :   0, tr_loss : 0.648\n",
      "epochs :   1, tr_loss : 0.598\n",
      "epochs :   2, tr_loss : 0.553\n",
      "epochs :   3, tr_loss : 0.513\n",
      "epochs :   4, tr_loss : 0.475\n",
      "epochs :   5, tr_loss : 0.438\n",
      "epochs :   6, tr_loss : 0.400\n",
      "epochs :   7, tr_loss : 0.360\n",
      "epochs :   8, tr_loss : 0.317\n",
      "epochs :   9, tr_loss : 0.274\n",
      "epochs :  10, tr_loss : 0.233\n",
      "epochs :  11, tr_loss : 0.194\n",
      "epochs :  12, tr_loss : 0.159\n",
      "epochs :  13, tr_loss : 0.128\n",
      "epochs :  14, tr_loss : 0.101\n",
      "epochs :  15, tr_loss : 0.078\n",
      "epochs :  16, tr_loss : 0.058\n",
      "epochs :  17, tr_loss : 0.043\n",
      "epochs :  18, tr_loss : 0.031\n",
      "epochs :  19, tr_loss : 0.022\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    _, tr_loss = adam_solver.train(sess = sess, seq_len = words_len, seq_batch = word_batch, y_data = y_data, lr = 1e-2)\n",
    "    print('epochs : {:3}, tr_loss : {:.3f}'.format(epoch, tr_loss))\n",
    "    loss_history.append(tr_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 100.00%\n"
     ]
    }
   ],
   "source": [
    "yhat = char_gru.predict(sess = sess, seq_len = words_len, seq_batch = word_batch)\n",
    "print('Accuracy : {:.2%}'.format(np.mean(np.argmax(y_data, axis = 1) == yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze embeddings of words\n",
    "영어단어를 왼쪽부터 오른쪽으로 알파벳 한글자씩보고, 긍/부정을 예측하는 모형을 CharRNN으로 학습한 후, 이 때 특정 영어단어를 넣었을 때의 hidden state를 비교하면 아래와 같은 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'bad', 'amazing', 'so good', 'bull shit']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = char_gru.encode(sess = sess, seq_len = words_len, seq_batch = word_batch)\n",
    "embedding = pd.DataFrame(embedding_words, index = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>amazing</th>\n",
       "      <th>so good</th>\n",
       "      <th>bull shit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.090620</td>\n",
       "      <td>0.383285</td>\n",
       "      <td>0.357408</td>\n",
       "      <td>-0.126503</td>\n",
       "      <td>0.689105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.142947</td>\n",
       "      <td>0.140611</td>\n",
       "      <td>-0.120752</td>\n",
       "      <td>-0.174374</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.481865</td>\n",
       "      <td>-0.201977</td>\n",
       "      <td>0.569966</td>\n",
       "      <td>0.677262</td>\n",
       "      <td>-0.394927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.490165</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.646111</td>\n",
       "      <td>-0.665153</td>\n",
       "      <td>0.374838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.408214</td>\n",
       "      <td>-0.081710</td>\n",
       "      <td>0.662239</td>\n",
       "      <td>0.600297</td>\n",
       "      <td>-0.258617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.552937</td>\n",
       "      <td>-0.156794</td>\n",
       "      <td>0.672020</td>\n",
       "      <td>0.731237</td>\n",
       "      <td>-0.695990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.251143</td>\n",
       "      <td>-0.242771</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.399336</td>\n",
       "      <td>-0.654192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.167962</td>\n",
       "      <td>-0.333250</td>\n",
       "      <td>0.383378</td>\n",
       "      <td>0.284998</td>\n",
       "      <td>-0.732360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.355938</td>\n",
       "      <td>-0.166141</td>\n",
       "      <td>0.671152</td>\n",
       "      <td>0.543169</td>\n",
       "      <td>-0.490243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.268323</td>\n",
       "      <td>0.256304</td>\n",
       "      <td>-0.575321</td>\n",
       "      <td>-0.463283</td>\n",
       "      <td>0.471359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.306457</td>\n",
       "      <td>0.130883</td>\n",
       "      <td>0.084107</td>\n",
       "      <td>0.680491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.572170</td>\n",
       "      <td>-0.262817</td>\n",
       "      <td>0.595968</td>\n",
       "      <td>0.769292</td>\n",
       "      <td>-0.515272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.308290</td>\n",
       "      <td>-0.159760</td>\n",
       "      <td>0.471211</td>\n",
       "      <td>0.472356</td>\n",
       "      <td>-0.558880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.225386</td>\n",
       "      <td>-0.004079</td>\n",
       "      <td>-0.074401</td>\n",
       "      <td>0.241343</td>\n",
       "      <td>-0.447123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.106748</td>\n",
       "      <td>0.259758</td>\n",
       "      <td>0.296636</td>\n",
       "      <td>-0.079479</td>\n",
       "      <td>0.769076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.499670</td>\n",
       "      <td>-0.162387</td>\n",
       "      <td>0.752082</td>\n",
       "      <td>0.692628</td>\n",
       "      <td>-0.520880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.129184</td>\n",
       "      <td>0.744328</td>\n",
       "      <td>0.774138</td>\n",
       "      <td>-0.563597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.123847</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>-0.534177</td>\n",
       "      <td>-0.253004</td>\n",
       "      <td>0.099086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.312512</td>\n",
       "      <td>-0.158462</td>\n",
       "      <td>0.491121</td>\n",
       "      <td>0.459040</td>\n",
       "      <td>-0.522646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.627392</td>\n",
       "      <td>0.175276</td>\n",
       "      <td>-0.503636</td>\n",
       "      <td>-0.785335</td>\n",
       "      <td>0.522767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        good       bad   amazing   so good  bull shit\n",
       "0  -0.090620  0.383285  0.357408 -0.126503   0.689105\n",
       "1  -0.142947  0.140611 -0.120752 -0.174374   0.740000\n",
       "2   0.481865 -0.201977  0.569966  0.677262  -0.394927\n",
       "3  -0.490165  0.012184 -0.646111 -0.665153   0.374838\n",
       "4   0.408214 -0.081710  0.662239  0.600297  -0.258617\n",
       "5   0.552937 -0.156794  0.672020  0.731237  -0.695990\n",
       "6   0.251143 -0.242771  0.563700  0.399336  -0.654192\n",
       "7   0.167962 -0.333250  0.383378  0.284998  -0.732360\n",
       "8   0.355938 -0.166141  0.671152  0.543169  -0.490243\n",
       "9  -0.268323  0.256304 -0.575321 -0.463283   0.471359\n",
       "10  0.001902  0.306457  0.130883  0.084107   0.680491\n",
       "11  0.572170 -0.262817  0.595968  0.769292  -0.515272\n",
       "12  0.308290 -0.159760  0.471211  0.472356  -0.558880\n",
       "13  0.225386 -0.004079 -0.074401  0.241343  -0.447123\n",
       "14 -0.106748  0.259758  0.296636 -0.079479   0.769076\n",
       "15  0.499670 -0.162387  0.752082  0.692628  -0.520880\n",
       "16  0.583276 -0.129184  0.744328  0.774138  -0.563597\n",
       "17 -0.123847  0.036050 -0.534177 -0.253004   0.099086\n",
       "18  0.312512 -0.158462  0.491121  0.459040  -0.522646\n",
       "19 -0.627392  0.175276 -0.503636 -0.785335   0.522767"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어의 20차원 embedding\n",
    "embedding.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding된 각 단어간의 euclidean distance를 계산해보면,***같은 범주 (긍정 또는 부정)에 속한 단어들 끼리 가깝고, 서로 다른 범주에 속한 단어들 끼리는 상대적으로 먼것을 확인 가능***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>amazing</th>\n",
       "      <th>so good</th>\n",
       "      <th>bull shit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.417702</td>\n",
       "      <td>1.118611</td>\n",
       "      <td>0.677330</td>\n",
       "      <td>3.980407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>2.417702</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>2.979147</td>\n",
       "      <td>3.047030</td>\n",
       "      <td>1.678281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>1.118611</td>\n",
       "      <td>2.979147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871289</td>\n",
       "      <td>4.414713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so good</th>\n",
       "      <td>0.677330</td>\n",
       "      <td>3.047030</td>\n",
       "      <td>0.871289</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>4.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bull shit</th>\n",
       "      <td>3.980407</td>\n",
       "      <td>1.678281</td>\n",
       "      <td>4.414713</td>\n",
       "      <td>4.573600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               good       bad   amazing   so good  bull shit\n",
       "good       0.000000  2.417702  1.118611  0.677330   3.980407\n",
       "bad        2.417702  0.000345  2.979147  3.047030   1.678281\n",
       "amazing    1.118611  2.979147  0.000000  0.871289   4.414713\n",
       "so good    0.677330  3.047030  0.871289  0.000977   4.573600\n",
       "bull shit  3.980407  1.678281  4.414713  4.573600   0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "pd.DataFrame(pairwise_distances(X = embedding), index = words, columns= words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode unseen words\n",
    "Input을 Character 단위로 받으므로 학습에 이용되지않은 영어단어에 대해서도 embedding을 구할 수 있으며, 긍/부정을 판단할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = ['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_len, unseen_batch = make_batch(unseen, 10)\n",
    "unseen_vector = char_gru.encode(sess = sess, seq_len = unseen_len, seq_batch = unseen_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36647743, -0.01373214,  0.10402276, -0.1385455 , -0.00311775,\n",
       "         0.09648833, -0.14816383, -0.11707296,  0.2829062 , -0.02193693,\n",
       "         0.10115166,  0.08888885,  0.00755516, -0.02079342,  0.18498814,\n",
       "         0.04492051,  0.04183176, -0.15769707, -0.01859909,  0.01101905]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_gru.predict(sess = sess, seq_len = unseen_len, seq_batch = unseen_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
