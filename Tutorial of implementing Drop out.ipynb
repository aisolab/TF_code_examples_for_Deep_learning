{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of implementing Drop out\n",
    "mnist image를 분류하는 Convolution Neural Network에 Drop out을 적용하는 간단한 example\n",
    "\n",
    "Drop out paper : http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data # load mnist dataset\n",
    "mnist = input_data.read_data_sets(train_dir = './MNIST_data', one_hot = True, reshape = True, seed = 777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MnistCNN class\n",
    "conv-conv-max pool-conv-conv-max pool-fc-fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop out의 구현을 위해서는 tf.nn module에 있는 tf.nn.dropout을 활용한다. tf.nn.dropout의 wrapper로 \n",
    "tf.layers.dropout이 있지만 해당 api는 default rate가 정해져있고, dropout을 걸지 안 걸지를 결정하기위해서\n",
    "training argument가 존재하는데, 두 가지를 다 조절하려면 두 개의 tf.placeholder가 필요하기 때문에 개인 선호에 의해서\n",
    "tf.nn.dropout을 활용한다.(tf.nn.dropout은 keep_prob argument 하나만 존재)\n",
    "일반적인 Convolution Neural Network에 적용할 때는, 마지막 output layer (label의 개수와 같은 fc layer) 전의 fc layer에\n",
    "Drop out을 적용한다.\n",
    "(Convolution Neural Network의 Filter에 적용하는 논문도 있다. 이는 drop connect 논문과 비슷한 발상에서 나온 것 같다)\n",
    "'''\n",
    "class MnistCNN:\n",
    "    def __init__(self, activation_fn = tf.nn.relu, initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "                 l2_scale = 0.01):\n",
    "        \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            self._x = tf.placeholder(dtype = tf.float32, shape = [None,784])\n",
    "            self._ximg = tf.reshape(tensor = self._x, shape = [-1,28,28,1])\n",
    "            self._y = tf.placeholder(dtype = tf.float32, shape = [None,10])\n",
    "            self._keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "            \n",
    "        with tf.variable_scope('conv_layer1'):\n",
    "            _conv_pre = tf.layers.conv2d(inputs = self._ximg, filters = 64, kernel_size = [3,3],\n",
    "                                        kernel_initializer = initializer,\n",
    "                                        kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale),\n",
    "                                        padding = 'same')\n",
    "            _conv_relu = activation_fn(_conv_pre)\n",
    "            \n",
    "        with tf.variable_scope('conv_layer2'):\n",
    "            _conv_pre = tf.layers.conv2d(inputs = _conv_relu, filters = 64, kernel_size = [3,3],\n",
    "                                        kernel_initializer = initializer,\n",
    "                                        kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale),\n",
    "                                        padding = 'same')\n",
    "            _conv_relu = activation_fn(_conv_pre)\n",
    "            \n",
    "        with tf.variable_scope('max_pool1'):\n",
    "            _pooled = tf.layers.max_pooling2d(inputs = _conv_relu, pool_size = [2,2], strides = 2)\n",
    "            \n",
    "        with tf.variable_scope('conv_layer3'):\n",
    "            _conv_pre = tf.layers.conv2d(inputs = _pooled, filters = 128, kernel_size = [3,3],\n",
    "                                        kernel_initializer = initializer,\n",
    "                                        kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale),\n",
    "                                        padding = 'same')\n",
    "            _conv_relu = activation_fn(_conv_pre)\n",
    "            \n",
    "        with tf.variable_scope('conv_layer4'):\n",
    "            _conv_pre = tf.layers.conv2d(inputs = _conv_relu, filters = 128, kernel_size = [3,3],\n",
    "                                        kernel_initializer = initializer,\n",
    "                                        kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale),\n",
    "                                        padding = 'same')\n",
    "            _conv_relu = activation_fn(_conv_pre)\n",
    "            \n",
    "        with tf.variable_scope('max_pool2'):\n",
    "            _pooled = tf.layers.max_pooling2d(inputs = _conv_relu, pool_size = [2,2], strides = 2)\n",
    "        \n",
    "        with tf.variable_scope('dense_layer1'):\n",
    "            _pooled_vector = tf.reshape(tensor = _pooled, shape = [-1,np.cumprod(_pooled.get_shape().as_list()[-3:])[-1]])\n",
    "            _fc_pre = tf.layers.dense(inputs = _pooled_vector, units = 1024, kernel_initializer = initializer,\n",
    "                                  kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale))\n",
    "            _fc_relu = activation_fn(_fc_pre)\n",
    "            _fc_relu = tf.nn.dropout(x = _fc_relu, keep_prob = self._keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('output_layer'):\n",
    "            self._score = tf.layers.dense(inputs = _fc_relu, units = 10, kernel_initializer = initializer,\n",
    "                                          kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = l2_scale))\n",
    "            \n",
    "        with tf.variable_scope('loss'):\n",
    "            _ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self._score, labels = self._y))\n",
    "            _reg_term = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            self._total_loss = _ce_loss +  _reg_term\n",
    "\n",
    "                \n",
    "        with tf.variable_scope('predict'):\n",
    "            self._prediction = tf.argmax(input = self._score, axis = 1)\n",
    "    \n",
    "    def predict(self, sess, x_data, keep_prob = 1.):\n",
    "        feed_predict = {self._x : x_data, self._keep_prob : keep_prob}\n",
    "        return sess.run(fetches = self._prediction, feed_dict = feed_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Solver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, model, optimizer = tf.train.AdamOptimizer, var_list = None):\n",
    "        self._model = model\n",
    "        self._lr = tf.placeholder(dtype = tf.float32)\n",
    "        self._optimizer = optimizer(learning_rate = self._lr)\n",
    "        self._training_op = self._optimizer.minimize(loss = self._model._total_loss, var_list = var_list)\n",
    "    \n",
    "    def train(self, sess, x_data, y_data, lr, keep_prob = .5):\n",
    "        feed_train = {self._model._x : x_data, self._model._y : y_data, self._lr : lr,\n",
    "                      self._model._keep_prob : keep_prob}\n",
    "        return sess.run(fetches = [self._training_op, self._model._total_loss], feed_dict = feed_train)\n",
    "            \n",
    "    def evaluate(self, sess, x_data, y_data, keep_prob = 1.):\n",
    "        feed_loss = {self._model._x : x_data, self._model._y : y_data, self._model._keep_prob : keep_prob}\n",
    "        return sess.run(fetches = self._model._total_loss, feed_dict = feed_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate CNN model and Adam solver### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "mnist_classifier = MnistCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_solver = Solver(model = mnist_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypear-parameters\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "best_loss = np.infty\n",
    "max_checks_without_progress = 15\n",
    "checks_without_progress = 0\n",
    "tr_loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :   0, tr_loss : 17.423, val_loss : 21.375\n",
      "step : 100, tr_loss : 3.446, val_loss : 3.483\n",
      "step : 200, tr_loss : 1.886, val_loss : 1.836\n",
      "step : 300, tr_loss : 1.265, val_loss : 1.269\n",
      "step : 400, tr_loss : 0.894, val_loss : 0.996\n",
      "step : 500, tr_loss : 0.744, val_loss : 0.723\n",
      "epoch :   0, tr_loss : 2.493, val_loss : 2.422\n",
      "step :   0, tr_loss : 0.694, val_loss : 0.737\n",
      "step : 100, tr_loss : 0.577, val_loss : 0.590\n",
      "step : 200, tr_loss : 0.454, val_loss : 0.454\n",
      "step : 300, tr_loss : 0.468, val_loss : 0.478\n",
      "step : 400, tr_loss : 0.370, val_loss : 0.361\n",
      "step : 500, tr_loss : 0.317, val_loss : 0.388\n",
      "epoch :   1, tr_loss : 0.488, val_loss : 0.461\n",
      "step :   0, tr_loss : 0.351, val_loss : 0.377\n",
      "step : 100, tr_loss : 0.323, val_loss : 0.360\n",
      "step : 200, tr_loss : 0.275, val_loss : 0.270\n",
      "step : 300, tr_loss : 0.303, val_loss : 0.257\n",
      "step : 400, tr_loss : 0.315, val_loss : 0.312\n",
      "step : 500, tr_loss : 0.263, val_loss : 0.273\n",
      "epoch :   2, tr_loss : 0.335, val_loss : 0.311\n",
      "step :   0, tr_loss : 0.295, val_loss : 0.326\n",
      "step : 100, tr_loss : 0.335, val_loss : 0.345\n",
      "step : 200, tr_loss : 0.253, val_loss : 0.306\n",
      "step : 300, tr_loss : 0.319, val_loss : 0.270\n",
      "step : 400, tr_loss : 0.246, val_loss : 0.224\n",
      "step : 500, tr_loss : 0.305, val_loss : 0.273\n",
      "epoch :   3, tr_loss : 0.303, val_loss : 0.281\n",
      "step :   0, tr_loss : 0.278, val_loss : 0.259\n",
      "step : 100, tr_loss : 0.333, val_loss : 0.243\n",
      "step : 200, tr_loss : 0.318, val_loss : 0.272\n",
      "step : 300, tr_loss : 0.282, val_loss : 0.282\n",
      "step : 400, tr_loss : 0.389, val_loss : 0.251\n",
      "step : 500, tr_loss : 0.281, val_loss : 0.403\n",
      "epoch :   4, tr_loss : 0.295, val_loss : 0.271\n",
      "step :   0, tr_loss : 0.296, val_loss : 0.351\n",
      "step : 100, tr_loss : 0.270, val_loss : 0.348\n",
      "step : 200, tr_loss : 0.346, val_loss : 0.374\n",
      "step : 300, tr_loss : 0.228, val_loss : 0.232\n",
      "step : 400, tr_loss : 0.254, val_loss : 0.274\n",
      "step : 500, tr_loss : 0.259, val_loss : 0.210\n",
      "epoch :   5, tr_loss : 0.287, val_loss : 0.263\n",
      "step :   0, tr_loss : 0.265, val_loss : 0.226\n",
      "step : 100, tr_loss : 0.439, val_loss : 0.263\n",
      "step : 200, tr_loss : 0.275, val_loss : 0.250\n",
      "step : 300, tr_loss : 0.251, val_loss : 0.316\n",
      "step : 400, tr_loss : 0.366, val_loss : 0.274\n",
      "step : 500, tr_loss : 0.457, val_loss : 0.251\n",
      "epoch :   6, tr_loss : 0.285, val_loss : 0.259\n",
      "step :   0, tr_loss : 0.259, val_loss : 0.260\n",
      "step : 100, tr_loss : 0.300, val_loss : 0.225\n",
      "step : 200, tr_loss : 0.457, val_loss : 0.301\n",
      "step : 300, tr_loss : 0.267, val_loss : 0.251\n",
      "step : 400, tr_loss : 0.278, val_loss : 0.237\n",
      "step : 500, tr_loss : 0.244, val_loss : 0.277\n",
      "epoch :   7, tr_loss : 0.281, val_loss : 0.255\n",
      "step :   0, tr_loss : 0.308, val_loss : 0.231\n",
      "step : 100, tr_loss : 0.205, val_loss : 0.281\n",
      "step : 200, tr_loss : 0.228, val_loss : 0.225\n",
      "step : 300, tr_loss : 0.296, val_loss : 0.216\n",
      "step : 400, tr_loss : 0.284, val_loss : 0.275\n",
      "step : 500, tr_loss : 0.265, val_loss : 0.258\n",
      "epoch :   8, tr_loss : 0.279, val_loss : 0.252\n",
      "step :   0, tr_loss : 0.254, val_loss : 0.184\n",
      "step : 100, tr_loss : 0.291, val_loss : 0.252\n",
      "step : 200, tr_loss : 0.228, val_loss : 0.234\n",
      "step : 300, tr_loss : 0.228, val_loss : 0.241\n",
      "step : 400, tr_loss : 0.301, val_loss : 0.271\n",
      "step : 500, tr_loss : 0.239, val_loss : 0.273\n",
      "epoch :   9, tr_loss : 0.274, val_loss : 0.250\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    avg_tr_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for step in range(total_batch):\n",
    "        \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size = batch_size)\n",
    "        val_xs, val_ys = mnist.validation.next_batch(batch_size = batch_size)\n",
    "        _, tr_loss = adam_solver.train(sess = sess, x_data = batch_xs, y_data = batch_ys, lr = 1e-3, keep_prob = .5)\n",
    "        val_loss = adam_solver.evaluate(sess = sess, x_data = val_xs, y_data = val_ys, keep_prob = 1)\n",
    "        \n",
    "        avg_tr_loss += tr_loss / total_batch\n",
    "        avg_val_loss += val_loss / total_batch\n",
    "        if step % 100 == 0:\n",
    "            print('step : {:3}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(step, tr_loss, val_loss))\n",
    "    \n",
    "    print('epoch : {:3}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch, avg_tr_loss, avg_val_loss))\n",
    "    tr_loss_history.append(avg_tr_loss)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    \n",
    "     # early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        checks_without_progress = 0\n",
    "    else:\n",
    "        checks_without_progress += 1\n",
    "        if checks_without_progress > max_checks_without_progress:\n",
    "            print('Early stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25a0e474f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25a0e5fc588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_loss_history, label = 'train')\n",
    "plt.plot(val_loss_history, label = 'validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat = mnist_classifier.predict(sess=sess, x_data=mnist.test.images, keep_prob=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 97.85%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy : {:.2%}'.format(np.mean(np.argmax(mnist.test.labels, axis = 1) == hat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
